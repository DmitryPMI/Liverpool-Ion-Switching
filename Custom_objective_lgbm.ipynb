{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Передача своей целевой функции в модель LGBM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_train = pd.read_csv('regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('regression.test', header=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>fobj</b> (callable or None, optional (default=None)) –\n",
    "\n",
    "Customized objective function. Should accept two parameters: <b>preds</b>, <b>train_data</b>, and return (grad, hess).\n",
    "\n",
    "predslist or numpy 1-D array\n",
    "The predicted values.\n",
    "\n",
    "train_dataDataset\n",
    "The training dataset.\n",
    "\n",
    "gradlist or numpy 1-D array\n",
    "The value of the first order derivative (gradient) for each sample point.\n",
    "\n",
    "hesslist or numpy 1-D array\n",
    "The value of the second order derivative (Hessian) for each sample point.\n",
    "\n",
    "For binary task, the preds is margin. For multi-class task, the preds is group by class_id first, then group by row_id. If you want to get i-th row preds in j-th class, the access way is score [j * num_data + i] and you should group grad and hess in this way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_logistic_obj(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    p = y_hat \n",
    "    grad = 4 * p * y + p - 5 * y\n",
    "    hess = (4 * y + 1) * (p * (1.0 - p))\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>feval</b> (callable or None, optional (default=None)) –\n",
    "\n",
    "Customized evaluation function. Should accept two parameters: <b>preds</b>, <b>train_data</b>, and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n",
    "\n",
    "predslist or numpy 1-D array\n",
    "The predicted values.\n",
    "\n",
    "train_dataDataset\n",
    "The training dataset.\n",
    "\n",
    "eval_namestring\n",
    "The name of evaluation function (without whitespaces).\n",
    "\n",
    "eval_resultfloat\n",
    "The eval result.\n",
    "\n",
    "is_higher_betterbool\n",
    "Is eval result higher better, e.g. AUC is is_higher_better.\n",
    "\n",
    "For binary task, the preds is probability of positive class (or margin in case of specified fobj). For multi-class task, the preds is group by class_id first, then group by row_id. If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i]. To ignore the default metric corresponding to the used objective, set the metric parameter to the string \"None\" in params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_err_rate(y_hat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    y_hat = np.clip(y_hat, 10e-7, 1-10e-7)\n",
    "    loss_fn = y*np.log(y_hat)\n",
    "    loss_fp = (1.0 - y)*np.log(1.0 - y_hat)\n",
    "    return 'myloss', np.sum(-(5*loss_fn+loss_fp))/len(y), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'None', #'None'\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "[3]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "[4]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "[5]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "[6]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 0.544\tvalid_0's l1: 0.544\tvalid_0's myloss: 37.5782\n",
      "The rmse of prediction is: 0.737563556583431\n"
     ]
    }
   ],
   "source": [
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=40,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5,\n",
    "                fobj = my_logistic_obj,\n",
    "                feval = my_err_rate)\n",
    "\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss for Ion Switching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим сделать больший вес последним наблюдениям.\n",
    "\n",
    "$$ y,\\hat{y} \\in \\{0, \\dots, 9\\} \\\\ $$\n",
    "$$ Loss_1(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i = \\hat{y}_i ) (y_i + 1) \\\\ $$\n",
    "$$ Loss_2(y, \\hat{y}) = \\sum_{k=0}^{10} (k + 1) f_1(y, \\hat{y}) |\\quad  y = k \\\\ $$\n",
    "\n",
    "Эти функции потерь отражают то, что мы хотим, но они не нормированны.\n",
    "\n",
    "$$ Loss_3(y, \\hat{y}) = \\frac{(t + 2)(t +1)}{2}\\sum_{k=0}^{t} (k + 1) f_1(y, \\hat{y}) |\\quad  y = k \\\\ $$\n",
    "\n",
    "У этой функции есть проблемма: её долго считать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
